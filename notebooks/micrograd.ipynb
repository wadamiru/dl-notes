{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dca7940-710e-4334-a5e4-6e4f92b2db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl-notes | wad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566836fd-30c9-4884-9565-aef1f64a4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- micrograd from scratch impl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32d59f4d-4c3a-4d28-bfef-0e35709ce16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import math\n",
    "import random\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd76d35a-bb6c-4260-a82e-8eab44df0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node: holds scalar & gradient\n",
    "class Node:\n",
    "    def __init__(self, data, _prev=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_prev)\n",
    "        self._op = _op\n",
    "        self._backward = lambda:None\n",
    "\n",
    "    # addition\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Node) else Node(other)\n",
    "        out = Node(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            # d p += d child\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward # assign func. sign.\n",
    "\n",
    "        return out\n",
    "\n",
    "    # multiplication\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Node) else Node(other)\n",
    "        out = Node(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            # d p1 += d child * d p2\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # power\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Only supporting int, float powers\"\n",
    "        out = Node(self.data ** other.data, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            # d p += d child * (pw * x**(pw-1))\n",
    "            self.grad += out.grad * (other * self.data**(other-1))\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # activation: ReLU\n",
    "    def relu(self):\n",
    "        out = Node(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            # d x = 0(if out = 0), out.grad(if out != 0)\n",
    "            self.grad += (out.data>0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # main backprop\n",
    "    def backward(self):\n",
    "        topo = [] # topology (ordered)\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v) # add parent node after its children\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1 # init. last parent.grad to 1\n",
    "        for v in reversed(topo): # start from the end\n",
    "            v._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d88e9f8b-6f63-4ce4-97ae-00b9f6bdd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn module: module/neurone/layer/mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "642759f5-7d90-4c2b-925e-700b8d6f2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class\n",
    "class Module(ABC):\n",
    "    def zero_grad(self): # reset gradient to zero\n",
    "        for p in self.params():\n",
    "            p.grad = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def params(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3161602-4648-4c5d-9da1-9c747c04c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurone\n",
    "class Neurone(Module):\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Node(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Node(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.z = sum((wi*xi for wi,xi in zip(self.w, x)), self.b) # {w1x1 + wnxn + b}\n",
    "        return z.relu() if self.nonlin else z\n",
    "\n",
    "    def params(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neurone({len(self.w)})\" # LinearNeurone(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21bfbfa0-1268-47dc-8689-6656c5384912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer\n",
    "class Layer(Module):\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurones = [Neurone(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurones]\n",
    "        return out[0] if len(out)==1 else out\n",
    "\n",
    "    def params(self):\n",
    "        return [p for n in self.neurones for p in n.params()] # single list of params\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurones)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97b60f83-cacf-4268-a308-82067f1e678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        ls = [nin] + nouts # all layers: nin(in-layer), nouts(outs of every layer)\n",
    "        self.layers = [Layer(ls[i], ls[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))] # nonlin=True except last layer\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def params(self):\n",
    "        return [p for l in self.layers for p in l.params()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        sep = \",\\n\\t\" # seperator\n",
    "        return f\"MLP of [\\n\\t{sep.join(str(l) for l in self.layers)}\\n]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea295819-a3db-4c4a-88dc-3ff2c4240e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP of [\n",
      "\tLayer of [ReLUNeurone(2), ReLUNeurone(2), ReLUNeurone(2)],\n",
      "\tLayer of [ReLUNeurone(3), ReLUNeurone(3), ReLUNeurone(3)],\n",
      "\tLayer of [LinearNeurone(3)]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(2, [3,3,1])\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cad80-d179-4bdf-aa96-874a948ca2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
